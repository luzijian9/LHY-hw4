{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Iallen520/lhy_DL_Hw/blob/master/hw4_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>连接</a>\n",
    "\n",
    "根据这个改进：https://www.cnblogs.com/dogecheng/p/11565530.html 标点符号 停用词\n",
    "\n",
    "1）Word2Vec使用了无标签数据集导致词表过大\n",
    "\n",
    "2）训练10个epoch 测试集没提升，训练集到acc到90%——>过拟合 （训练集19万和18万没区别）\n",
    "\n",
    "3）网络（加多一个全连接），测试集没有提升——>数据的问题\n",
    "\n",
    "4）原来的数据can ' t存在空格，所以用txt的替换功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\jupyter item\\\\LHY_RNN'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r67y9UpchZ38"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "本次作業是要讓同學接觸NLP當中一個簡單的task——句子分類(文本分類)\n",
    "\n",
    "給定一個句子，判斷他有沒有惡意(負面標1，正面標0)\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ajS_WskRo0S"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# path_prefix = 'drive/My Drive/Colab Notebooks/hw4 - Recurrent Neural Network'\n",
    "path_prefix = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrAlczfM_w6"
   },
   "source": [
    "### Download Dataset\n",
    "有三個檔案，分別是 training_label.txt、training_nolabel.txt、testing_data.txt\n",
    "\n",
    "- training_label.txt：有 label 的 training data（句子配上 0 or 1，+++$+++ 只是分隔符號，不要理它）\n",
    "    - e.g., 1 +++$+++ are wtf ... awww thanks !\n",
    "\n",
    "- training_nolabel.txt：沒有 label 的 training data（只有句子），用來做 semi-supervised learning\n",
    "    - ex: hates being this burnt !! ouch\n",
    "\n",
    "- testing_data.txt：你要判斷 testing data 裡面的句子是 0 or 1\n",
    "\n",
    "    >id,text\n",
    "\n",
    "    >0,my dog ate our dinner . no , seriously ... he ate it .\n",
    "\n",
    "    >1,omg last day sooon n of primary noooooo x im gona be swimming out of school wif the amount of tears am gona cry\n",
    "\n",
    "    >2,stupid boys .. they ' re so .. stupid !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "x2gwKORmuViJ",
    "outputId": "4db7c645-cd05-4f90-8826-e9a45d40440b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8\n",
      "To: /content/data.zip\n",
      "45.1MB [00:00, 203MB/s]\n",
      "Archive:  data.zip\n",
      "  inflating: training_label.txt      \n",
      "  inflating: testing_data.txt        \n",
      "  inflating: training_nolabel.txt    \n",
      "data.zip     testing_data.txt\t training_nolabel.txt\n",
      "sample_data  training_label.txt\n"
     ]
    }
   ],
   "source": [
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1dPHIl8ZnfDz_fxNd2ZeBYedTat2lfxcO' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/training_label.txt'\n",
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1x1rJOX_ETqnOZjdMAbEE2pqIjRNa8xcc' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/training_nolabel.txt'\n",
    "# !wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=16CtnQwSDCob9xmm6EdHHR7PNFNiOrQ30' -O 'drive/My Drive/Colab Notebooks/hw8-RNN/data/testing_data.txt'\n",
    "\n",
    "!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip #Google Drive直链工具\n",
    "!unzip data.zip\n",
    "!ls\n",
    "\n",
    "# !ls 'drive/My Drive/Colab Notebooks/hw4 - Recurrent Neural Network/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hDIokoP6464"
   },
   "outputs": [],
   "source": [
    "# this is for filtering the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 怎么读取数据的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"0  wish weekend  but not really also  cuz next monday is exam and i haven't studied at all yet hate exam  grr\", \"1  check this vid out  you'll piss yourself laughin\", '0  damn you gavin !!!!!! i want my computer back !!!!', '0  got a bloody wheel clamp yesterday ï ¿ ½150 for 15 mins parking', \"0  homework and summer school  we'll go soon though !\"]\n",
      "[['wish', 'weekend', 'but', 'not', 'really', 'also', 'cuz', 'next', 'monday', 'is', 'exam', 'and', 'i', \"haven't\", 'studied', 'at', 'all', 'yet', 'hate', 'exam', 'grr'], ['check', 'this', 'vid', 'out', \"you'll\", 'piss', 'yourself', 'laughin'], ['damn', 'you', 'gavin', '!!!!!!', 'i', 'want', 'my', 'computer', 'back', '!!!!'], ['got', 'bloody', 'wheel', 'clamp', 'yesterday', '½150', 'for', '15', 'mins', 'parking'], ['homework', 'and', 'summer', 'school', \"we'll\", 'go', 'soon', 'though', '!']]\n",
      "['0', '1', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "#把testing時需要的data讀進來\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "stop = '[‘’\"#$%&\\()*+,./:;<=>@[\\\\]^_`{|}~]+'\n",
    "# with open('./test.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "# #     print(lines)  \n",
    "#     X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]#split()如果句子有多个逗号就会分成几句。需要join()来连接\n",
    "#     print(X)\n",
    "#     X = [re.sub(stop, \"\", sentence).strip() for sentence in X]\n",
    "#     X = [sen.split() for sen in X]\n",
    "#     print(X)\n",
    "stop_words = set(text.ENGLISH_STOP_WORDS)\n",
    "# print(stop_words)\n",
    "line = []\n",
    "line1 = []\n",
    "with open('./train.txt', 'r', encoding='UTF-8') as f: #需要encoding='UTF-8'\n",
    "    lines = f.readlines()\n",
    "    X = [re.sub(stop, \"\", sentence).strip() for sentence in lines]\n",
    "    print(X)\n",
    "#     lines = [line.strip('\\n').split() for line in X]\n",
    "    for sentence in X:\n",
    "        label = sentence[0]\n",
    "        tmp = [word for word in sentence[1:].strip('\\n').split() if len(word)>1 or word in ['i','!','?']  ] # if word not in stop_words\n",
    "        line.append(tmp)\n",
    "        line1.append(label)\n",
    "x = line\n",
    "y = line1\n",
    "\n",
    "#     lines = f.readlines()\n",
    "#     lines = [line.strip('\\n').split(' ') for line in lines]\n",
    "# x = [line[2:] for line in lines]\n",
    "# y = [line[0] for line in lines]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 2]\n",
      " [3 3 3]\n",
      " [4 4 4]\n",
      " [1 1 1]]\n",
      "[3 1 2 0]\n",
      "[[4 4 4]\n",
      " [2 2 2]\n",
      " [3 3 3]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(111)\n",
    "lines = [\n",
    "    [1,1,1],\n",
    "    [2,2,2],\n",
    "    [3,3,3],\n",
    "    [4,4,4],\n",
    "]\n",
    "lines1 = np.array(lines)\n",
    "lines2 = np.array(lines)\n",
    "np.random.shuffle(lines1)\n",
    "index = np.random.permutation(len(lines2))\n",
    "print(lines1)\n",
    "print(index)\n",
    "print(lines2[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fc143hSvNGr6"
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICDIhhgCY2-M"
   },
   "outputs": [],
   "source": [
    "# utils.py\n",
    "# 這個block用來先定義一些等等常用到的函式\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "stop = '[’\"#$%&\\()*+,./:;<=>@[\\\\]^_`{|}~]+'\n",
    "def load_training_data(path='training_label.txt'):\n",
    "    # 把training時需要的data讀進來\n",
    "    # 如果是'training_label.txt'，需要讀取label，如果是'training_nolabel.txt'，不需要讀取label\n",
    "    global stop\n",
    "    line=[]\n",
    "    line1=[]\n",
    "    if 'training_label' in path:\n",
    "        with open(path, 'r', encoding='UTF-8') as f: #需要encoding='UTF-8'\n",
    "#             lines = f.readlines()\n",
    "#             X = [re.sub(stop, \"\", sentence).strip() for sentence in lines]\n",
    "#             lines = [line.strip('\\n').split() for line in X]\n",
    "#         x = [line[1:] for line in lines]\n",
    "#         y = [line[0] for line in lines]\n",
    "            lines = f.readlines()\n",
    "            X = [re.sub(stop, \"\", sentence).strip() for sentence in lines]\n",
    "            for sentence in X:\n",
    "                label = sentence[0]\n",
    "                tmp = [word for word in sentence[1:].strip('\\n').split() if len(word)>1 or word in ['i','!','?']]\n",
    "                line.append(tmp)\n",
    "                line1.append(label)\n",
    "        print(len(line1))\n",
    "        return line, line1\n",
    "    else:\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            lines = f.readlines()\n",
    "            X = [re.sub(stop, \"\", sentence).strip() for sentence in lines]\n",
    "            lines = [line.strip('\\n').split() for line in X]\n",
    "        return lines\n",
    "\n",
    "def load_testing_data(path='testing_data'):\n",
    "    # 把testing時需要的data讀進來\n",
    "    global stop\n",
    "    line=[]\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
    "        X = [re.sub(stop, \"\", sentence).strip() for sentence in X]\n",
    "#         X = [sen.split() for sen in X]\n",
    "        for sentence in X:\n",
    "            tmp = [word for word in sentence.strip('\\n').split() if len(word)>1 or word in ['i','!','?']]\n",
    "            line.append(tmp)\n",
    "        print(len(line))\n",
    "    return line\n",
    "\n",
    "def evaluation(outputs, labels):\n",
    "    #outputs => probability (float)\n",
    "    #labels => labels\n",
    "    outputs[outputs>=0.5] = 1 # 大於等於0.5為有惡意\n",
    "    outputs[outputs<0.5] = 0 # 小於0.5為無惡意\n",
    "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYE8UYQsNIxM"
   },
   "source": [
    "### Train Word to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "cgGWaF8_2S3q",
    "outputId": "d25efb6b-de4f-43f2-8a98-e0aaa47f704e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data ...\n",
      "200000\n",
      "loading testing data ...\n",
      "200000\n",
      "saving model ...\n"
     ]
    }
   ],
   "source": [
    "# w2v.py\n",
    "# 這個block是用來訓練word to vector 的 word embedding\n",
    "# 注意！這個block在訓練word to vector時是用cpu，可能要花到10分鐘以上\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def train_word2vec(x):\n",
    "    # 訓練word to vector 的 word embedding\n",
    "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1) #sg0：CBOW sg1：skip\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"loading training data ...\")\n",
    "    train_x, y = load_training_data('./hw4/training_label.txt')\n",
    "#     train_x_no_label = load_training_data('./hw4/training_nolabel.txt')\n",
    "\n",
    "    print(\"loading testing data ...\")\n",
    "    test_x = load_testing_data('./hw4/testing_data.txt')\n",
    "\n",
    "    model = train_word2vec(train_x  + test_x) #+ train_x_no_label\n",
    "    \n",
    "    print(\"saving model ...\")\n",
    "    # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n",
    "    model.save(os.path.join(path_prefix, 'w2v_all.model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wHLtS0wNR6w"
   },
   "source": [
    "### Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfGKiOitk5ob"
   },
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "# 這個block用來做data的預處理\n",
    "from torch import nn\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "    def get_w2v_model(self):\n",
    "        # 把之前訓練好的word to vec 模型讀進來\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "    def add_embedding(self, word):\n",
    "        # 把word加進embedding，並賦予他一個隨機生成的representation vector\n",
    "        # word只會是\"<PAD>\"或\"<UNK>\"\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        # 取得訓練好的 Word2vec word embedding\n",
    "        if load:\n",
    "            print(\"loading word to vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # 製作一個 word2idx 的 dictionary\n",
    "        # 製作一個 idx2word 的 list\n",
    "        # 製作一個 word2vector 的 list\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "            print('get words #{}'.format(i+1), end='\\r') #\\r表示回到行首\n",
    "            #e.g. self.word2index['魯'] = 1 \n",
    "            #e.g. self.index2word[1] = '魯'\n",
    "            #e.g. self.vectors[1] = '魯' vector\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        print('')\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        # 將\"<PAD>\"跟\"<UNK>\"加進embedding裡面\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "    def pad_sequence(self, sentence):\n",
    "        # 將每個句子變成一樣的長度\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    def sentence_word2idx(self):\n",
    "        # 把句子裡面的字轉成相對應的index\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            print('sentence count #{}'.format(i+1), end='\\r')\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "            # 將每個句子變成一樣的長度\n",
    "            sentence_idx = self.pad_sequence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    def labels_to_tensor(self, y):\n",
    "        # 把labels轉成tensor\n",
    "        y = [int(label) for label in y] #int()说明y是一维\n",
    "        return torch.LongTensor(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3WJB7go5NWL0"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XketwKs4lFfB"
   },
   "outputs": [],
   "source": [
    "# data.py\n",
    "# 實作了dataset所需要的'__init__', '__getitem__', '__len__'\n",
    "# 好讓dataloader能使用\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class TwitterDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    Data can be a list of numpy array or a list of lists\n",
    "    input data shape : (data_num, seq_len, feature_dim)\n",
    "    \n",
    "    __len__ will return the number of data\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        return self.data[idx], self.label[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Embedding\n",
    "\n",
    "所以Embedding会增加一个维度，即feature_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2278, -1.5511, -0.1098,  0.7454,  1.2229]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "tensor([[-0.2278, -1.5511, -0.1098,  0.7454,  1.2229],\n",
      "        [ 1.7156,  1.3619,  0.2018, -1.0366,  1.3712],\n",
      "        [-0.7197,  1.7748,  0.7382,  1.1914,  1.0706],\n",
      "        [ 1.7156,  1.3619,  0.2018, -1.0366,  1.3712]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "word2idx = {'hello':0, 'world':1, 'goodbye':2}\n",
    "embeds = nn.Embedding(3,5)\n",
    "hello_idx = torch.LongTensor([word2idx['hello']])\n",
    "hello_idx = Variable(hello_idx)\n",
    "hello_embed = embeds(hello_idx)\n",
    "print(hello_embed)\n",
    "random_idx = torch.LongTensor([0,2,1,2])\n",
    "random_idx = Variable(random_idx)\n",
    "random_embed = embeds(random_idx)\n",
    "print(random_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "24711\n",
      "Vocab(count:17820, index:37, sample_int:3475032967)\n",
      "[ 0.1594226   0.15222673  0.11124863 -0.1402431   0.13791953 -0.20665413\n",
      " -0.19474867  0.23798528  0.10662556  0.1699507 ]\n",
      "[ 0.1594226   0.15222673  0.11124863 -0.1402431   0.13791953 -0.20665413\n",
      " -0.19474867  0.23798528  0.10662556  0.1699507 ]\n",
      "(24711, 250)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embedding = Word2Vec.load('./w2v_all.model')\n",
    "embedding_dim = embedding.vector_size\n",
    "print(embedding_dim)\n",
    "print(len(embedding.wv.vocab))\n",
    "print(embedding.wv.vocab['can'])\n",
    "print(embedding.wv['?'][:10])\n",
    "print(embedding['?'][0:10])\n",
    "print(embedding.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNJ8xWIMNa2r"
   },
   "source": [
    "## Model\n",
    "\n",
    "version0 : num_layers=1 dropout=0.5\n",
    "\n",
    "version1 : num_layers=2 dropout=0.3\n",
    "\n",
    "version2 : num_layers=2 dropout=0.5 add Linear（hidden->50,50->1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZS6RJADulIq1"
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "# 這個block是要拿來訓練的模型\n",
    "#embedding_dim=250, hidden_dim=250, num_layers=1\n",
    "import torch\n",
    "from torch import nn\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # 製作 embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        # 是否將 embedding fix住，如果fix_embedding為False，在訓練過程中，embedding也會跟著被訓練\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True,  bidirectional=True)\n",
    "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
    "                                         nn.Linear(500, 1),\n",
    "                                         nn.Sigmoid() )\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x 的 dimension (batch, seq_len, hidden_size)\n",
    "        # 取用 LSTM 最後一層的 hidden state\n",
    "        x = x[:, -1, :] \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWlpEL0sNc10"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QR4MMz-lR7i"
   },
   "outputs": [],
   "source": [
    "# train.py\n",
    "# 這個block是用來訓練模型的\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
    "    model.train() # 將model的模式設為train，這樣optimizer就可以更新model的參數\n",
    "    criterion = nn.BCELoss() # 定義損失函數，這裡我們使用binary cross entropy loss\n",
    "    t_batch = len(train)\n",
    "    \n",
    "    print('t_batch: {} \\n'.format(t_batch))\n",
    "    \n",
    "    v_batch = len(valid) \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給optimizer，並給予適當的learning rate\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss, total_acc = 0, 0\n",
    "        # 這段做training\n",
    "        for i, (inputs, labels) in enumerate(train):\n",
    "            inputs = inputs.to(device, dtype=torch.long) # device為\"cuda\"，將inputs轉成torch.cuda.LongTensor\n",
    "            labels = labels.to(device, dtype=torch.float) # device為\"cuda\"，將labels轉成torch.cuda.FloatTensor，因為等等要餵進criterion，所以型態要是float\n",
    "            optimizer.zero_grad() # 由於loss.backward()的gradient會累加，所以每次餵完一個batch後需要歸零\n",
    "            outputs = model(inputs) # 將input餵給模型\n",
    "            \n",
    "            #input的shape： (batch,seq_len)\n",
    "            #output的shape： (batch, hidden_size = 1)\n",
    "            \n",
    "            outputs = outputs.squeeze() # 去掉最外面的dimension，好讓outputs可以餵進criterion()\n",
    "            loss = criterion(outputs, labels) # 計算此時模型的training loss\n",
    "            loss.backward() # 算loss的gradient\n",
    "            optimizer.step() # 更新訓練模型的參數\n",
    "            \n",
    "            #evaluation在utils\n",
    "            correct = evaluation(outputs, labels) # 計算此時模型的training accuracy\n",
    "            total_acc += (correct / batch_size)\n",
    "            total_loss += loss.item()\n",
    "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
    "                epoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
    "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "\n",
    "        # 這段做validation\n",
    "        model.eval() # 將model的模式設為eval，這樣model的參數就會固定住\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (inputs, labels) in enumerate(valid):\n",
    "                inputs = inputs.to(device, dtype=torch.long) # device為\"cuda\"，將inputs轉成torch.cuda.LongTensor\n",
    "                labels = labels.to(device, dtype=torch.float) # device為\"cuda\"，將labels轉成torch.cuda.FloatTensor，因為等等要餵進criterion，所以型態要是float\n",
    "                outputs = model(inputs) # 將input餵給模型\n",
    "                outputs = outputs.squeeze() # 去掉最外面的dimension，好讓outputs可以餵進criterion()\n",
    "                loss = criterion(outputs, labels) # 計算此時模型的validation loss\n",
    "                correct = evaluation(outputs, labels) # 計算此時模型的validation accuracy\n",
    "                total_acc += (correct / batch_size)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "            if total_acc > best_acc:\n",
    "                # 如果validation的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n",
    "                best_acc = total_acc\n",
    "                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
    "                torch.save(model, \"{}/version1_ckpt.model\".format(model_dir))\n",
    "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
    "        print('-----------------------------------------------')\n",
    "        model.train() # 將model的模式設為train，這樣optimizer就可以更新model的參數（因為剛剛轉成eval模式）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qF5YQrupNfCS"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2X2wkdAYxHYA"
   },
   "outputs": [],
   "source": [
    "# test.py\n",
    "# 這個block用來對testing_data.txt做預測\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def testing(batch_size, test_loader, model, device):\n",
    "    model.eval()\n",
    "    ret_output = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1 # 大於等於0.5為負面\n",
    "            outputs[outputs<0.5] = 0 # 小於0.5為正面\n",
    "            ret_output += outputs.int().tolist()\n",
    "    \n",
    "    return ret_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfnKj0KXNeoz"
   },
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "EztIWqCmlZof",
    "outputId": "961deee1-4726-445b-cd49-ab4fcc0e14cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "200000\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n",
      "get words #24711\n",
      "total words: 24713\n",
      "sentence count #200000\n",
      "start training, parameter total:8686751, trainable:2508501\n",
      "\n",
      "t_batch: 1485 \n",
      "\n",
      "[ Epoch1: 1485/1485 ] loss:0.467 acc:29.688 \n",
      "Train | Loss:0.48580 Acc: 75.561\n",
      "Valid | Loss:0.44565 Acc: 77.690 \n",
      "saving model with acc 77.690\n",
      "-----------------------------------------------\n",
      "[ Epoch2: 1485/1485 ] loss:0.390 acc:29.688 \n",
      "Train | Loss:0.41803 Acc: 80.735\n",
      "Valid | Loss:0.42092 Acc: 79.480 \n",
      "saving model with acc 79.480\n",
      "-----------------------------------------------\n",
      "[ Epoch3: 1485/1485 ] loss:0.340 acc:30.469 \n",
      "Train | Loss:0.39915 Acc: 81.729\n",
      "Valid | Loss:0.41202 Acc: 80.498 \n",
      "saving model with acc 80.498\n",
      "-----------------------------------------------\n",
      "[ Epoch4: 1485/1485 ] loss:0.297 acc:31.250 \n",
      "Train | Loss:0.38159 Acc: 82.743\n",
      "Valid | Loss:0.41244 Acc: 80.390 \n",
      "-----------------------------------------------\n",
      "[ Epoch5: 1485/1485 ] loss:0.420 acc:29.688 \n",
      "Train | Loss:0.36126 Acc: 83.752\n",
      "Valid | Loss:0.40416 Acc: 80.726 \n",
      "saving model with acc 80.726\n",
      "-----------------------------------------------\n",
      "[ Epoch6: 92/1485 ] loss:0.314 acc:85.938 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-eb814dffb7d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;31m# 開始訓練\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-843c4d45f846>\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(batch_size, n_epoch, lr, model_dir, train, valid, model, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 去掉最外面的dimension，好讓outputs可以餵進criterion()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 計算此時模型的training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 算loss的gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 更新訓練模型的參數\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2112\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[1;32m-> 2113\u001b[1;33m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[0;32m   2114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 通過torch.cuda.is_available()的回傳值進行判斷是否有使用GPU的環境，如果有的話device就設為\"cuda\"，沒有的話就設為\"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 處理好各個data的路徑\n",
    "train_with_label = os.path.join(path_prefix, 'hw4/training_label.txt')\n",
    "# train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
    "testing_data = os.path.join(path_prefix, 'hw4/testing_data.txt')\n",
    "\n",
    "w2v_path = os.path.join(path_prefix, 'w2v_all.model') # 處理word to vec model的路徑\n",
    "\n",
    "# 定義句子長度、要不要固定embedding、batch大小、要訓練幾個epoch、learning rate的值、model的資料夾路徑\n",
    "sen_len = 30\n",
    "fix_embedding = True # fix embedding during training\n",
    "batch_size = 128\n",
    "epoch = 7\n",
    "lr = 0.001\n",
    "# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n",
    "model_dir = './hw4/logs/' #path_prefix # model directory for checkpoint model\n",
    "\n",
    "print(\"loading data ...\") # 把'training_label.txt'跟'training_nolabel.txt'讀進來\n",
    "train_x, y = load_training_data(train_with_label)\n",
    "# train_x_no_label = load_training_data(train_no_label)\n",
    "\n",
    "# 對input跟labels做預處理\n",
    "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True) #self.embedding_matrix\n",
    "train_x = preprocess.sentence_word2idx()\n",
    "y = preprocess.labels_to_tensor(y)\n",
    "\n",
    "# 製作一個model的對象\n",
    "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=250, num_layers=2, dropout=0.5,fix_embedding=fix_embedding)\n",
    "model = model.to(device) # device為\"cuda\"，model使用GPU來訓練(餵進去的inputs也需要是cuda tensor)\n",
    "\n",
    "# 把data分為training data跟validation data(將一部份training data拿去當作validation data)\n",
    "X_train, X_val, y_train, y_val = train_x[:190000], train_x[190000:], y[:190000], y[190000:]\n",
    "\n",
    "# 把data做成dataset供dataloader取用\n",
    "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
    "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
    "\n",
    "# 把data 轉成 batch of tensors\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            ) #num_workers = 8\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            ) #num_workers = 8\n",
    "\n",
    "# 開始訓練\n",
    "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fQeaQNeNm3L"
   },
   "source": [
    "### Predict and Write to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "vFvjFQopxVrt",
    "outputId": "3bf30696-bf76-463b-fc7f-f0b6375a0e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing data ...\n",
      "Get embedding ...\n",
      "loading word to vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get words #1\r",
      "get words #2\r",
      "get words #3\r",
      "get words #4\r",
      "get words #5\r",
      "get words #6\r",
      "get words #7\r",
      "get words #8\r",
      "get words #9\r",
      "get words #10\r",
      "get words #11\r",
      "get words #12\r",
      "get words #13\r",
      "get words #14\r",
      "get words #15\r",
      "get words #16\r",
      "get words #17\r",
      "get words #18\r",
      "get words #19\r",
      "get words #20\r",
      "get words #21\r",
      "get words #22\r",
      "get words #23\r",
      "get words #24\r",
      "get words #25\r",
      "get words #26\r",
      "get words #27\r",
      "get words #28\r",
      "get words #29\r",
      "get words #30\r",
      "get words #31\r",
      "get words #32\r",
      "get words #33\r",
      "get words #34\r",
      "get words #35\r",
      "get words #36\r",
      "get words #37\r",
      "get words #38\r",
      "get words #39\r",
      "get words #40\r",
      "get words #41\r",
      "get words #42\r",
      "get words #43\r",
      "get words #44\r",
      "get words #45\r",
      "get words #46\r",
      "get words #47\r",
      "get words #48\r",
      "get words #49\r",
      "get words #50\r",
      "get words #51\r",
      "get words #52\r",
      "get words #53\r",
      "get words #54\r",
      "get words #55\r",
      "get words #56\r",
      "get words #57\r",
      "get words #58\r",
      "get words #59\r",
      "get words #60\r",
      "get words #61\r",
      "get words #62\r",
      "get words #63\r",
      "get words #64\r",
      "get words #65\r",
      "get words #66\r",
      "get words #67\r",
      "get words #68\r",
      "get words #69\r",
      "get words #70\r",
      "get words #71\r",
      "get words #72\r",
      "get words #73\r",
      "get words #74\r",
      "get words #75\r",
      "get words #76\r",
      "get words #77\r",
      "get words #78\r",
      "get words #79\r",
      "get words #80\r",
      "get words #81\r",
      "get words #82\r",
      "get words #83\r",
      "get words #84\r",
      "get words #85\r",
      "get words #86\r",
      "get words #87\r",
      "get words #88\r",
      "get words #89\r",
      "get words #90\r",
      "get words #91\r",
      "get words #92\r",
      "get words #93\r",
      "get words #94\r",
      "get words #95\r",
      "get words #96\r",
      "get words #97\r",
      "get words #98\r",
      "get words #99\r",
      "get words #100\r",
      "get words #101\r",
      "get words #102\r",
      "get words #103\r",
      "get words #104\r",
      "get words #105\r",
      "get words #106\r",
      "get words #107\r",
      "get words #108\r",
      "get words #109\r",
      "get words #110\r",
      "get words #111\r",
      "get words #112\r",
      "get words #113\r",
      "get words #114\r",
      "get words #115\r",
      "get words #116\r",
      "get words #117\r",
      "get words #118\r",
      "get words #119\r",
      "get words #120\r",
      "get words #121\r",
      "get words #122\r",
      "get words #123\r",
      "get words #124\r",
      "get words #125\r",
      "get words #126\r",
      "get words #127\r",
      "get words #128\r",
      "get words #129\r",
      "get words #130\r",
      "get words #131\r",
      "get words #132\r",
      "get words #133\r",
      "get words #134\r",
      "get words #135\r",
      "get words #136\r",
      "get words #137\r",
      "get words #138\r",
      "get words #139\r",
      "get words #140\r",
      "get words #141\r",
      "get words #142\r",
      "get words #143\r",
      "get words #144\r",
      "get words #145\r",
      "get words #146\r",
      "get words #147\r",
      "get words #148\r",
      "get words #149\r",
      "get words #150\r",
      "get words #151\r",
      "get words #152\r",
      "get words #153\r",
      "get words #154\r",
      "get words #155\r",
      "get words #156\r",
      "get words #157\r",
      "get words #158\r",
      "get words #159\r",
      "get words #160\r",
      "get words #161\r",
      "get words #162\r",
      "get words #163\r",
      "get words #164\r",
      "get words #165\r",
      "get words #166\r",
      "get words #167\r",
      "get words #168\r",
      "get words #169\r",
      "get words #170\r",
      "get words #171\r",
      "get words #172\r",
      "get words #173\r",
      "get words #174\r",
      "get words #175\r",
      "get words #176\r",
      "get words #177\r",
      "get words #178\r",
      "get words #179\r",
      "get words #180\r",
      "get words #181\r",
      "get words #182\r",
      "get words #183\r",
      "get words #184\r",
      "get words #185\r",
      "get words #186\r",
      "get words #187\r",
      "get words #188\r",
      "get words #189\r",
      "get words #190\r",
      "get words #191\r",
      "get words #192\r",
      "get words #193\r",
      "get words #194\r",
      "get words #195\r",
      "get words #196\r",
      "get words #197\r",
      "get words #198\r",
      "get words #199\r",
      "get words #200\r",
      "get words #201\r",
      "get words #202\r",
      "get words #203\r",
      "get words #204\r",
      "get words #205\r",
      "get words #206\r",
      "get words #207\r",
      "get words #208\r",
      "get words #209\r",
      "get words #210\r",
      "get words #211\r",
      "get words #212\r",
      "get words #213\r",
      "get words #214\r",
      "get words #215\r",
      "get words #216\r",
      "get words #217\r",
      "get words #218\r",
      "get words #219\r",
      "get words #220\r",
      "get words #221\r",
      "get words #222\r",
      "get words #223\r",
      "get words #224\r",
      "get words #225\r",
      "get words #226\r",
      "get words #227\r",
      "get words #228\r",
      "get words #229\r",
      "get words #230\r",
      "get words #231\r",
      "get words #232\r",
      "get words #233\r",
      "get words #234\r",
      "get words #235\r",
      "get words #236\r",
      "get words #237\r",
      "get words #238\r",
      "get words #239\r",
      "get words #240\r",
      "get words #241\r",
      "get words #242\r",
      "get words #243\r",
      "get words #244\r",
      "get words #245\r",
      "get words #246\r",
      "get words #247\r",
      "get words #248\r",
      "get words #249\r",
      "get words #250\r",
      "get words #251\r",
      "get words #252\r",
      "get words #253\r",
      "get words #254\r",
      "get words #255\r",
      "get words #256\r",
      "get words #257\r",
      "get words #258\r",
      "get words #259\r",
      "get words #260\r",
      "get words #261\r",
      "get words #262\r",
      "get words #263\r",
      "get words #264\r",
      "get words #265\r",
      "get words #266\r",
      "get words #267\r",
      "get words #268\r",
      "get words #269\r",
      "get words #270\r",
      "get words #271\r",
      "get words #272\r",
      "get words #273\r",
      "get words #274\r",
      "get words #275\r",
      "get words #276\r",
      "get words #277\r",
      "get words #278\r",
      "get words #279\r",
      "get words #280\r",
      "get words #281\r",
      "get words #282\r",
      "get words #283\r",
      "get words #284\r",
      "get words #285\r",
      "get words #286\r",
      "get words #287\r",
      "get words #288\r",
      "get words #289\r",
      "get words #290\r",
      "get words #291\r",
      "get words #292\r",
      "get words #293\r",
      "get words #294\r",
      "get words #295\r",
      "get words #296\r",
      "get words #297\r",
      "get words #298\r",
      "get words #299\r",
      "get words #300\r",
      "get words #301\r",
      "get words #302\r",
      "get words #303\r",
      "get words #304\r",
      "get words #305\r",
      "get words #306\r",
      "get words #307\r",
      "get words #308\r",
      "get words #309\r",
      "get words #310\r",
      "get words #311\r",
      "get words #312\r",
      "get words #313\r",
      "get words #314\r",
      "get words #315\r",
      "get words #316\r",
      "get words #317\r",
      "get words #318\r",
      "get words #319\r",
      "get words #320\r",
      "get words #321\r",
      "get words #322\r",
      "get words #323\r",
      "get words #324\r",
      "get words #325\r",
      "get words #326\r",
      "get words #327\r",
      "get words #328\r",
      "get words #329\r",
      "get words #330\r",
      "get words #331\r",
      "get words #332\r",
      "get words #333\r",
      "get words #334\r",
      "get words #335\r",
      "get words #336\r",
      "get words #337\r",
      "get words #338\r",
      "get words #339\r",
      "get words #340\r",
      "get words #341\r",
      "get words #342\r",
      "get words #343\r",
      "get words #344\r",
      "get words #345\r",
      "get words #346\r",
      "get words #347\r",
      "get words #348\r",
      "get words #349\r",
      "get words #350\r",
      "get words #351\r",
      "get words #352\r",
      "get words #353\r",
      "get words #354\r",
      "get words #355\r",
      "get words #356\r",
      "get words #357\r",
      "get words #358\r",
      "get words #359\r",
      "get words #360\r",
      "get words #361\r",
      "get words #362\r",
      "get words #363\r",
      "get words #364\r",
      "get words #365\r",
      "get words #366\r",
      "get words #367\r",
      "get words #368\r",
      "get words #369\r",
      "get words #370\r",
      "get words #371\r",
      "get words #372\r",
      "get words #373\r",
      "get words #374\r",
      "get words #375\r",
      "get words #376\r",
      "get words #377\r",
      "get words #378\r",
      "get words #379\r",
      "get words #380\r",
      "get words #381\r",
      "get words #382\r",
      "get words #383\r",
      "get words #384\r",
      "get words #385\r",
      "get words #386\r",
      "get words #387\r",
      "get words #388\r",
      "get words #389\r",
      "get words #390\r",
      "get words #391\r",
      "get words #392\r",
      "get words #393\r",
      "get words #394\r",
      "get words #395\r",
      "get words #396\r",
      "get words #397\r",
      "get words #398\r",
      "get words #399\r",
      "get words #400\r",
      "get words #401\r",
      "get words #402\r",
      "get words #403\r",
      "get words #404\r",
      "get words #405\r",
      "get words #406\r",
      "get words #407\r",
      "get words #408\r",
      "get words #409\r",
      "get words #410\r",
      "get words #411\r",
      "get words #412\r",
      "get words #413\r",
      "get words #414\r",
      "get words #415\r",
      "get words #416\r",
      "get words #417\r",
      "get words #418\r",
      "get words #419\r",
      "get words #420\r",
      "get words #421\r",
      "get words #422\r",
      "get words #423\r",
      "get words #424\r",
      "get words #425\r",
      "get words #426\r",
      "get words #427\r",
      "get words #428\r",
      "get words #429\r",
      "get words #430\r",
      "get words #431\r",
      "get words #432\r",
      "get words #433\r",
      "get words #434\r",
      "get words #435\r",
      "get words #436\r",
      "get words #437\r",
      "get words #438\r",
      "get words #439\r",
      "get words #440\r",
      "get words #441\r",
      "get words #442\r",
      "get words #443\r",
      "get words #444\r",
      "get words #445\r",
      "get words #446\r",
      "get words #447\r",
      "get words #448\r",
      "get words #449\r",
      "get words #450\r",
      "get words #451\r",
      "get words #452\r",
      "get words #453\r",
      "get words #454\r",
      "get words #455\r",
      "get words #456\r",
      "get words #457\r",
      "get words #458\r",
      "get words #459\r",
      "get words #460\r",
      "get words #461\r",
      "get words #462\r",
      "get words #463\r",
      "get words #464\r",
      "get words #465\r",
      "get words #466\r",
      "get words #467\r",
      "get words #468\r",
      "get words #469\r",
      "get words #470\r",
      "get words #471\r",
      "get words #472\r",
      "get words #473\r",
      "get words #474\r",
      "get words #475\r",
      "get words #476\r",
      "get words #477\r",
      "get words #478\r",
      "get words #479\r",
      "get words #480\r",
      "get words #481\r",
      "get words #482\r",
      "get words #483\r",
      "get words #484\r",
      "get words #485\r",
      "get words #486\r",
      "get words #487\r",
      "get words #488\r",
      "get words #489\r",
      "get words #490\r",
      "get words #491\r",
      "get words #492\r",
      "get words #493\r",
      "get words #494\r",
      "get words #495\r",
      "get words #496\r",
      "get words #497\r",
      "get words #498\r",
      "get words #499\r",
      "get words #500\r",
      "get words #501\r",
      "get words #502\r",
      "get words #503\r",
      "get words #504\r",
      "get words #505\r",
      "get words #506\r",
      "get words #507\r",
      "get words #508\r",
      "get words #509\r",
      "get words #510\r",
      "get words #511\r",
      "get words #512\r",
      "get words #513\r",
      "get words #514\r",
      "get words #515\r",
      "get words #516\r",
      "get words #517\r",
      "get words #518\r",
      "get words #519\r",
      "get words #520\r",
      "get words #521\r",
      "get words #522\r",
      "get words #523\r",
      "get words #524\r",
      "get words #525\r",
      "get words #526\r",
      "get words #527\r",
      "get words #528\r",
      "get words #529\r",
      "get words #530\r",
      "get words #531\r",
      "get words #532\r",
      "get words #533\r",
      "get words #534\r",
      "get words #535\r",
      "get words #536\r",
      "get words #537\r",
      "get words #538\r",
      "get words #539\r",
      "get words #540\r",
      "get words #541\r",
      "get words #542\r",
      "get words #543\r",
      "get words #544\r",
      "get words #545\r",
      "get words #546\r",
      "get words #547\r",
      "get words #548\r",
      "get words #549\r",
      "get words #550\r",
      "get words #551\r",
      "get words #552\r",
      "get words #553\r",
      "get words #554\r",
      "get words #555\r",
      "get words #556\r",
      "get words #557\r",
      "get words #558\r",
      "get words #559\r",
      "get words #560\r",
      "get words #561\r",
      "get words #562\r",
      "get words #563\r",
      "get words #564\r",
      "get words #565\r",
      "get words #566\r",
      "get words #567\r",
      "get words #568\r",
      "get words #569\r",
      "get words #570\r",
      "get words #571\r",
      "get words #572\r",
      "get words #573\r",
      "get words #574\r",
      "get words #575\r",
      "get words #576\r",
      "get words #577\r",
      "get words #578\r",
      "get words #579\r",
      "get words #580\r",
      "get words #581\r",
      "get words #582\r",
      "get words #583\r",
      "get words #584\r",
      "get words #585\r",
      "get words #586\r",
      "get words #587\r",
      "get words #588\r",
      "get words #589\r",
      "get words #590\r",
      "get words #591\r",
      "get words #592\r",
      "get words #593\r",
      "get words #594\r",
      "get words #595\r",
      "get words #596\r",
      "get words #597\r",
      "get words #598\r",
      "get words #599\r",
      "get words #600\r",
      "get words #601\r",
      "get words #602\r",
      "get words #603\r",
      "get words #604\r",
      "get words #605\r",
      "get words #606\r",
      "get words #607\r",
      "get words #608\r",
      "get words #609\r",
      "get words #610\r",
      "get words #611\r",
      "get words #612\r",
      "get words #613\r",
      "get words #614\r",
      "get words #615\r",
      "get words #616\r",
      "get words #617\r",
      "get words #618\r",
      "get words #619\r",
      "get words #620\r",
      "get words #621\r",
      "get words #622\r",
      "get words #623\r",
      "get words #624\r",
      "get words #625\r",
      "get words #626\r",
      "get words #627\r",
      "get words #628\r",
      "get words #629\r",
      "get words #630\r",
      "get words #631\r",
      "get words #632\r",
      "get words #633\r",
      "get words #634\r",
      "get words #635\r",
      "get words #636\r",
      "get words #637\r",
      "get words #638\r",
      "get words #639\r",
      "get words #640\r",
      "get words #641\r",
      "get words #642\r",
      "get words #643\r",
      "get words #644\r",
      "get words #645\r",
      "get words #646\r",
      "get words #647\r",
      "get words #648\r",
      "get words #649\r",
      "get words #650\r",
      "get words #651\r",
      "get words #652\r",
      "get words #653\r",
      "get words #654\r",
      "get words #655\r",
      "get words #656\r",
      "get words #657\r",
      "get words #658\r",
      "get words #659\r",
      "get words #660\r",
      "get words #661\r",
      "get words #662\r",
      "get words #663\r",
      "get words #664\r",
      "get words #665\r",
      "get words #666\r",
      "get words #667\r",
      "get words #668\r",
      "get words #669\r",
      "get words #670\r",
      "get words #671\r",
      "get words #672\r",
      "get words #673\r",
      "get words #674\r",
      "get words #675\r",
      "get words #676\r",
      "get words #677\r",
      "get words #678\r",
      "get words #679\r",
      "get words #680\r",
      "get words #681\r",
      "get words #682\r",
      "get words #683\r",
      "get words #684\r",
      "get words #685\r",
      "get words #686\r",
      "get words #687\r",
      "get words #688\r",
      "get words #689\r",
      "get words #690\r",
      "get words #691\r",
      "get words #692\r",
      "get words #693\r",
      "get words #694\r",
      "get words #695\r",
      "get words #696\r",
      "get words #697\r",
      "get words #698\r",
      "get words #699\r",
      "get words #700\r",
      "get words #701\r",
      "get words #702\r",
      "get words #703\r",
      "get words #704\r",
      "get words #705\r",
      "get words #706\r",
      "get words #707\r",
      "get words #708\r",
      "get words #709\r",
      "get words #710\r",
      "get words #711\r",
      "get words #712\r",
      "get words #713\r",
      "get words #714\r",
      "get words #715\r",
      "get words #716\r",
      "get words #717\r",
      "get words #718\r",
      "get words #719\r",
      "get words #720\r",
      "get words #721\r",
      "get words #722\r",
      "get words #723\r",
      "get words #724\r",
      "get words #725\r",
      "get words #726\r",
      "get words #727\r",
      "get words #728\r",
      "get words #729\r",
      "get words #730\r",
      "get words #731\r",
      "get words #732\r",
      "get words #733\r",
      "get words #734\r",
      "get words #735\r",
      "get words #736\r",
      "get words #737\r",
      "get words #738\r",
      "get words #739\r",
      "get words #740\r",
      "get words #741\r",
      "get words #742\r",
      "get words #743\r",
      "get words #744\r",
      "get words #745\r",
      "get words #746\r",
      "get words #747\r",
      "get words #748\r",
      "get words #749\r",
      "get words #750\r",
      "get words #751\r",
      "get words #752\r",
      "get words #753\r",
      "get words #754\r",
      "get words #755\r",
      "get words #756\r",
      "get words #757\r",
      "get words #758\r",
      "get words #759\r",
      "get words #760\r",
      "get words #761\r",
      "get words #762\r",
      "get words #763\r",
      "get words #764\r",
      "get words #765\r",
      "get words #766\r",
      "get words #767\r",
      "get words #768\r",
      "get words #769\r",
      "get words #770\r",
      "get words #771\r",
      "get words #772\r",
      "get words #773\r",
      "get words #774\r",
      "get words #775\r",
      "get words #776\r",
      "get words #777\r",
      "get words #778\r",
      "get words #779\r",
      "get words #780\r",
      "get words #781\r",
      "get words #782\r",
      "get words #783\r",
      "get words #784\r",
      "get words #785\r",
      "get words #786\r",
      "get words #787\r",
      "get words #788\r",
      "get words #789\r",
      "get words #790\r",
      "get words #791\r",
      "get words #792\r",
      "get words #793\r",
      "get words #794\r",
      "get words #795\r",
      "get words #796\r",
      "get words #797\r",
      "get words #798\r",
      "get words #799\r",
      "get words #800\r",
      "get words #801\r",
      "get words #802\r",
      "get words #803\r",
      "get words #804\r",
      "get words #805\r",
      "get words #806\r",
      "get words #807\r",
      "get words #808\r",
      "get words #809\r",
      "get words #810\r",
      "get words #811\r",
      "get words #812\r",
      "get words #813\r",
      "get words #814\r",
      "get words #815\r",
      "get words #816\r",
      "get words #817\r",
      "get words #818\r",
      "get words #819\r",
      "get words #820\r",
      "get words #821\r",
      "get words #822\r",
      "get words #823\r",
      "get words #824\r",
      "get words #825\r",
      "get words #826\r",
      "get words #827\r",
      "get words #828\r",
      "get words #829\r",
      "get words #830\r",
      "get words #831\r",
      "get words #832\r",
      "get words #833\r",
      "get words #834\r",
      "get words #835\r",
      "get words #836\r",
      "get words #837\r",
      "get words #838\r",
      "get words #839\r",
      "get words #840\r",
      "get words #841\r",
      "get words #842\r",
      "get words #843\r",
      "get words #844\r",
      "get words #845\r",
      "get words #846\r",
      "get words #847\r",
      "get words #848\r",
      "get words #849\r",
      "get words #850\r",
      "get words #851\r",
      "get words #852\r",
      "get words #853\r",
      "get words #854\r",
      "get words #855\r",
      "get words #856\r",
      "get words #857\r",
      "get words #858\r",
      "get words #859\r",
      "get words #860\r",
      "get words #861\r",
      "get words #862\r",
      "get words #863\r",
      "get words #864\r",
      "get words #865\r",
      "get words #866\r",
      "get words #867\r",
      "get words #868\r",
      "get words #869\r",
      "get words #870\r",
      "get words #871\r",
      "get words #872\r",
      "get words #873\r",
      "get words #874\r",
      "get words #875\r",
      "get words #876\r",
      "get words #877\r",
      "get words #878\r",
      "get words #879\r",
      "get words #880\r",
      "get words #881\r",
      "get words #882\r",
      "get words #883\r",
      "get words #884\r",
      "get words #885\r",
      "get words #886\r",
      "get words #887\r",
      "get words #888\r",
      "get words #889\r",
      "get words #890\r",
      "get words #891\r",
      "get words #892\r",
      "get words #893\r",
      "get words #894\r",
      "get words #895\r",
      "get words #896\r",
      "get words #897\r",
      "get words #898\r",
      "get words #899\r",
      "get words #900\r",
      "get words #901\r",
      "get words #902\r",
      "get words #903\r",
      "get words #904\r",
      "get words #905\r",
      "get words #906\r",
      "get words #907\r",
      "get words #908\r",
      "get words #909\r",
      "get words #910\r",
      "get words #911\r",
      "get words #912\r",
      "get words #913\r",
      "get words #914\r",
      "get words #915\r",
      "get words #916\r",
      "get words #917\r",
      "get words #918\r",
      "get words #919\r",
      "get words #920\r",
      "get words #921\r",
      "get words #922\r",
      "get words #923\r",
      "get words #924\r",
      "get words #925\r",
      "get words #926\r",
      "get words #927\r",
      "get words #928\r",
      "get words #929\r",
      "get words #930\r",
      "get words #931\r",
      "get words #932\r",
      "get words #933\r",
      "get words #934\r",
      "get words #935\r",
      "get words #936\r",
      "get words #937\r",
      "get words #938\r",
      "get words #939\r",
      "get words #940\r",
      "get words #941\r",
      "get words #942\r",
      "get words #943\r",
      "get words #944\r",
      "get words #945\r",
      "get words #946\r",
      "get words #947\r",
      "get words #948\r",
      "get words #949\r",
      "get words #950\r",
      "get words #951\r",
      "get words #952\r",
      "get words #953\r",
      "get words #954\r",
      "get words #955\r",
      "get words #956\r",
      "get words #957\r",
      "get words #958\r",
      "get words #959\r",
      "get words #960\r",
      "get words #961\r",
      "get words #962\r",
      "get words #963\r",
      "get words #964\r",
      "get words #965\r",
      "get words #966\r",
      "get words #967\r",
      "get words #968\r",
      "get words #969\r",
      "get words #970\r",
      "get words #971\r",
      "get words #972\r",
      "get words #973\r",
      "get words #974\r",
      "get words #975\r",
      "get words #976\r",
      "get words #977\r",
      "get words #978\r",
      "get words #979\r",
      "get words #980\r",
      "get words #981\r",
      "get words #982\r",
      "get words #983\r",
      "get words #984\r",
      "get words #985\r",
      "get words #986\r",
      "get words #987\r",
      "get words #988\r",
      "get words #989\r",
      "get words #990\r",
      "get words #991\r",
      "get words #992\r",
      "get words #993\r",
      "get words #994\r",
      "get words #995\r",
      "get words #996\r",
      "get words #997\r",
      "get words #998\r",
      "get words #999\r",
      "get words #1000\r",
      "get words #1001\r",
      "get words #1002\r",
      "get words #1003\r",
      "get words #1004\r",
      "get words #1005\r",
      "get words #1006\r",
      "get words #1007\r",
      "get words #1008\r",
      "get words #1009\r",
      "get words #1010\r",
      "get words #1011\r",
      "get words #1012\r",
      "get words #1013\r",
      "get words #1014\r",
      "get words #1015\r",
      "get words #1016\r",
      "get words #1017\r",
      "get words #1018\r",
      "get words #1019\r",
      "get words #1020\r",
      "get words #1021\r",
      "get words #1022\r",
      "get words #1023\r",
      "get words #1024\r",
      "get words #1025\r",
      "get words #1026\r",
      "get words #1027\r",
      "get words #1028\r",
      "get words #1029\r",
      "get words #1030\r",
      "get words #1031\r",
      "get words #1032\r",
      "get words #1033\r",
      "get words #1034\r",
      "get words #1035\r",
      "get words #1036\r",
      "get words #1037\r",
      "get words #1038\r",
      "get words #1039\r",
      "get words #1040\r",
      "get words #1041\r",
      "get words #1042\r",
      "get words #1043\r",
      "get words #1044\r",
      "get words #1045\r",
      "get words #1046\r",
      "get words #1047\r",
      "get words #1048\r",
      "get words #1049\r",
      "get words #1050\r",
      "get words #1051\r",
      "get words #1052\r",
      "get words #1053\r",
      "get words #1054\r",
      "get words #1055\r",
      "get words #1056\r",
      "get words #1057\r",
      "get words #1058\r",
      "get words #1059\r",
      "get words #1060\r",
      "get words #1061\r",
      "get words #1062\r",
      "get words #1063\r",
      "get words #1064\r",
      "get words #1065\r",
      "get words #1066\r",
      "get words #1067\r",
      "get words #1068\r",
      "get words #1069\r",
      "get words #1070\r",
      "get words #1071\r",
      "get words #1072\r",
      "get words #1073\r",
      "get words #1074\r",
      "get words #1075\r",
      "get words #1076\r",
      "get words #1077\r",
      "get words #1078\r",
      "get words #1079\r",
      "get words #1080\r",
      "get words #1081\r",
      "get words #1082\r",
      "get words #1083\r",
      "get words #1084\r",
      "get words #1085\r",
      "get words #1086\r",
      "get words #1087\r",
      "get words #1088\r",
      "get words #1089\r",
      "get words #1090\r",
      "get words #1091\r",
      "get words #1092\r",
      "get words #1093\r",
      "get words #1094\r",
      "get words #1095\r",
      "get words #1096\r",
      "get words #1097\r",
      "get words #1098\r",
      "get words #1099\r",
      "get words #1100\r",
      "get words #1101\r",
      "get words #1102\r",
      "get words #1103\r",
      "get words #1104\r",
      "get words #1105\r",
      "get words #1106\r",
      "get words #1107\r",
      "get words #1108\r",
      "get words #1109\r",
      "get words #1110\r",
      "get words #1111\r",
      "get words #1112\r",
      "get words #1113\r",
      "get words #1114\r",
      "get words #1115\r",
      "get words #1116\r",
      "get words #1117\r",
      "get words #1118\r",
      "get words #1119\r",
      "get words #1120\r",
      "get words #1121\r",
      "get words #1122\r",
      "get words #1123\r",
      "get words #1124\r",
      "get words #1125\r",
      "get words #1126\r",
      "get words #1127\r",
      "get words #1128\r",
      "get words #1129\r",
      "get words #1130\r",
      "get words #1131\r",
      "get words #1132\r",
      "get words #1133\r",
      "get words #1134\r",
      "get words #1135\r",
      "get words #1136\r",
      "get words #1137\r",
      "get words #1138\r",
      "get words #1139\r",
      "get words #1140\r",
      "get words #1141\r",
      "get words #1142\r",
      "get words #1143\r",
      "get words #1144\r",
      "get words #1145\r",
      "get words #1146\r",
      "get words #1147\r",
      "get words #1148\r",
      "get words #1149\r",
      "get words #1150\r",
      "get words #1151\r",
      "get words #1152\r",
      "get words #1153\r",
      "get words #1154\r",
      "get words #1155\r",
      "get words #1156\r",
      "get words #1157\r",
      "get words #1158\r",
      "get words #1159\r",
      "get words #1160\r",
      "get words #1161\r",
      "get words #1162\r",
      "get words #1163\r",
      "get words #1164\r",
      "get words #1165\r",
      "get words #1166\r",
      "get words #1167\r",
      "get words #1168\r",
      "get words #1169\r",
      "get words #1170\r",
      "get words #1171\r",
      "get words #1172\r",
      "get words #1173\r",
      "get words #1174\r",
      "get words #1175\r",
      "get words #1176\r",
      "get words #1177\r",
      "get words #1178\r",
      "get words #1179\r",
      "get words #1180\r",
      "get words #1181\r",
      "get words #1182\r",
      "get words #1183\r",
      "get words #1184\r",
      "get words #1185\r",
      "get words #1186\r",
      "get words #1187\r",
      "get words #1188\r",
      "get words #1189\r",
      "get words #1190\r",
      "get words #1191\r",
      "get words #1192\r",
      "get words #1193\r",
      "get words #1194\r",
      "get words #1195\r",
      "get words #1196\r",
      "get words #1197\r",
      "get words #1198\r",
      "get words #1199\r",
      "get words #1200\r",
      "get words #1201\r",
      "get words #1202\r",
      "get words #1203\r",
      "get words #1204\r",
      "get words #1205\r",
      "get words #1206\r",
      "get words #1207\r",
      "get words #1208\r",
      "get words #1209\r",
      "get words #1210\r",
      "get words #1211\r",
      "get words #1212\r",
      "get words #1213\r",
      "get words #1214\r",
      "get words #1215\r",
      "get words #1216\r",
      "get words #1217\r",
      "get words #1218\r",
      "get words #1219\r",
      "get words #1220\r",
      "get words #1221\r",
      "get words #1222\r",
      "get words #1223\r",
      "get words #1224\r",
      "get words #1225\r",
      "get words #1226\r",
      "get words #1227\r",
      "get words #1228\r",
      "get words #1229\r",
      "get words #1230\r",
      "get words #1231\r",
      "get words #1232\r",
      "get words #1233\r",
      "get words #1234\r",
      "get words #1235\r",
      "get words #1236\r",
      "get words #1237\r",
      "get words #1238\r",
      "get words #1239\r",
      "get words #1240\r",
      "get words #1241\r",
      "get words #1242\r",
      "get words #1243\r",
      "get words #1244\r",
      "get words #1245\r",
      "get words #1246\r",
      "get words #1247\r",
      "get words #1248\r",
      "get words #1249\r",
      "get words #1250\r",
      "get words #1251\r",
      "get words #1252\r",
      "get words #1253\r",
      "get words #1254\r",
      "get words #1255\r",
      "get words #1256\r",
      "get words #1257\r",
      "get words #1258\r",
      "get words #1259\r",
      "get words #1260\r",
      "get words #1261\r",
      "get words #1262\r",
      "get words #1263\r",
      "get words #1264\r",
      "get words #1265\r",
      "get words #1266\r",
      "get words #1267\r",
      "get words #1268\r",
      "get words #1269\r",
      "get words #1270\r",
      "get words #1271\r",
      "get words #1272\r",
      "get words #1273\r",
      "get words #1274\r",
      "get words #1275\r",
      "get words #1276\r",
      "get words #1277\r",
      "get words #1278\r",
      "get words #1279\r",
      "get words #1280\r",
      "get words #1281\r",
      "get words #1282\r",
      "get words #1283\r",
      "get words #1284\r",
      "get words #1285\r",
      "get words #1286\r",
      "get words #1287\r",
      "get words #1288\r",
      "get words #1289\r",
      "get words #1290\r",
      "get words #1291\r",
      "get words #1292\r",
      "get words #1293\r",
      "get words #1294\r",
      "get words #1295\r",
      "get words #1296\r",
      "get words #1297\r",
      "get words #1298\r",
      "get words #1299\r",
      "get words #1300\r",
      "get words #1301\r",
      "get words #1302\r",
      "get words #1303\r",
      "get words #1304\r",
      "get words #1305\r",
      "get words #1306\r",
      "get words #1307\r",
      "get words #1308\r",
      "get words #1309\r",
      "get words #1310\r",
      "get words #1311\r",
      "get words #1312\r",
      "get words #1313\r",
      "get words #1314\r",
      "get words #1315\r",
      "get words #1316\r",
      "get words #1317\r",
      "get words #1318\r",
      "get words #1319\r",
      "get words #1320\r",
      "get words #1321\r",
      "get words #1322\r",
      "get words #1323\r",
      "get words #1324\r",
      "get words #1325\r",
      "get words #1326\r",
      "get words #1327\r",
      "get words #1328\r",
      "get words #1329\r",
      "get words #1330\r",
      "get words #1331\r",
      "get words #1332\r",
      "get words #1333\r",
      "get words #1334\r",
      "get words #1335\r",
      "get words #1336\r",
      "get words #1337\r",
      "get words #1338\r",
      "get words #1339\r",
      "get words #1340\r",
      "get words #1341\r",
      "get words #1342\r",
      "get words #1343\r",
      "get words #1344\r",
      "get words #1345\r",
      "get words #1346\r",
      "get words #1347\r",
      "get words #1348\r",
      "get words #1349\r",
      "get words #1350\r",
      "get words #1351\r",
      "get words #1352\r",
      "get words #1353\r",
      "get words #1354\r",
      "get words #1355\r",
      "get words #1356\r",
      "get words #1357\r",
      "get words #1358\r",
      "get words #1359\r",
      "get words #1360\r",
      "get words #1361\r",
      "get words #1362\r",
      "get words #1363\r",
      "get words #1364\r",
      "get words #1365\r",
      "get words #1366\r",
      "get words #1367\r",
      "get words #1368\r",
      "get words #1369\r",
      "get words #1370\r",
      "get words #1371\r",
      "get words #1372\r",
      "get words #1373\r",
      "get words #1374\r",
      "get words #1375\r",
      "get words #1376\r",
      "get words #1377\r",
      "get words #1378\r",
      "get words #1379\r",
      "get words #1380\r",
      "get words #1381\r",
      "get words #1382\r",
      "get words #1383\r",
      "get words #1384\r",
      "get words #1385\r",
      "get words #1386\r",
      "get words #1387\r",
      "get words #1388\r",
      "get words #1389\r",
      "get words #1390\r",
      "get words #1391\r",
      "get words #1392\r",
      "get words #1393\r",
      "get words #1394\r",
      "get words #1395\r",
      "get words #1396\r",
      "get words #1397\r",
      "get words #1398\r",
      "get words #1399\r",
      "get words #1400\r",
      "get words #1401\r",
      "get words #1402\r",
      "get words #1403\r",
      "get words #1404\r",
      "get words #1405\r",
      "get words #1406\r",
      "get words #1407\r",
      "get words #1408\r",
      "get words #1409\r",
      "get words #1410\r",
      "get words #1411\r",
      "get words #1412\r",
      "get words #1413\r",
      "get words #1414\r",
      "get words #1415\r",
      "get words #1416\r",
      "get words #1417\r",
      "get words #1418\r",
      "get words #1419\r",
      "get words #1420\r",
      "get words #1421\r",
      "get words #1422\r",
      "get words #1423\r",
      "get words #1424\r",
      "get words #1425\r",
      "get words #1426\r",
      "get words #1427\r",
      "get words #1428\r",
      "get words #1429\r",
      "get words #1430\r",
      "get words #1431\r",
      "get words #1432\r",
      "get words #1433\r",
      "get words #1434\r",
      "get words #1435\r",
      "get words #1436\r",
      "get words #1437\r",
      "get words #1438\r",
      "get words #1439\r",
      "get words #1440\r",
      "get words #1441\r",
      "get words #1442\r",
      "get words #1443\r",
      "get words #1444\r",
      "get words #1445\r",
      "get words #1446\r",
      "get words #1447\r",
      "get words #1448\r",
      "get words #1449\r",
      "get words #1450\r",
      "get words #1451\r",
      "get words #1452\r",
      "get words #1453\r",
      "get words #1454\r",
      "get words #1455\r",
      "get words #1456\r",
      "get words #1457\r",
      "get words #1458\r",
      "get words #1459\r",
      "get words #1460\r",
      "get words #1461\r",
      "get words #1462\r",
      "get words #1463\r",
      "get words #1464\r",
      "get words #1465\r",
      "get words #1466\r",
      "get words #1467\r",
      "get words #1468\r",
      "get words #1469\r",
      "get words #1470\r",
      "get words #1471\r",
      "get words #1472\r",
      "get words #1473\r",
      "get words #1474\r",
      "get words #1475\r",
      "get words #1476\r",
      "get words #1477\r",
      "get words #1478\r",
      "get words #1479\r",
      "get words #1480\r",
      "get words #1481\r",
      "get words #1482\r",
      "get words #1483\r",
      "get words #1484\r",
      "get words #1485\r",
      "get words #1486\r",
      "get words #1487\r",
      "get words #1488\r",
      "get words #1489\r",
      "get words #1490\r",
      "get words #1491\r",
      "get words #1492\r",
      "get words #1493\r",
      "get words #1494\r",
      "get words #1495\r",
      "get words #1496\r",
      "get words #1497\r",
      "get words #1498\r",
      "get words #1499\r",
      "get words #1500\r",
      "get words #1501\r",
      "get words #1502\r",
      "get words #1503\r",
      "get words #1504\r",
      "get words #1505\r",
      "get words #1506\r",
      "get words #1507\r",
      "get words #1508\r",
      "get words #1509\r",
      "get words #1510\r",
      "get words #1511\r",
      "get words #1512\r",
      "get words #1513\r",
      "get words #1514\r",
      "get words #1515\r",
      "get words #1516\r",
      "get words #1517\r",
      "get words #1518\r",
      "get words #1519\r",
      "get words #1520\r",
      "get words #1521\r",
      "get words #1522\r",
      "get words #1523\r",
      "get words #1524\r",
      "get words #1525\r",
      "get words #1526\r",
      "get words #1527\r",
      "get words #1528\r",
      "get words #1529\r",
      "get words #1530\r",
      "get words #1531\r",
      "get words #1532\r",
      "get words #1533\r",
      "get words #1534\r",
      "get words #1535\r",
      "get words #1536\r",
      "get words #1537\r",
      "get words #1538\r",
      "get words #1539\r",
      "get words #1540\r",
      "get words #1541\r",
      "get words #1542\r",
      "get words #1543\r",
      "get words #1544\r",
      "get words #1545\r",
      "get words #1546\r",
      "get words #1547\r",
      "get words #1548\r",
      "get words #1549\r",
      "get words #1550\r",
      "get words #1551\r",
      "get words #1552\r",
      "get words #1553\r",
      "get words #1554\r",
      "get words #1555\r",
      "get words #1556\r",
      "get words #1557\r",
      "get words #1558\r",
      "get words #1559\r",
      "get words #1560\r",
      "get words #1561\r",
      "get words #1562\r",
      "get words #1563\r",
      "get words #1564\r",
      "get words #1565\r",
      "get words #1566\r",
      "get words #1567\r",
      "get words #1568\r",
      "get words #1569\r",
      "get words #1570\r",
      "get words #1571\r",
      "get words #1572\r",
      "get words #1573\r",
      "get words #1574\r",
      "get words #1575\r",
      "get words #1576\r",
      "get words #1577\r",
      "get words #1578\r",
      "get words #1579\r",
      "get words #1580\r",
      "get words #1581\r",
      "get words #1582\r",
      "get words #1583\r",
      "get words #1584\r",
      "get words #1585\r",
      "get words #1586\r",
      "get words #1587\r",
      "get words #1588\r",
      "get words #1589\r",
      "get words #1590\r",
      "get words #1591\r",
      "get words #1592\r",
      "get words #1593\r",
      "get words #1594\r",
      "get words #1595\r",
      "get words #1596\r",
      "get words #1597\r",
      "get words #1598\r",
      "get words #1599\r",
      "get words #1600\r",
      "get words #1601\r",
      "get words #1602\r",
      "get words #1603\r",
      "get words #1604\r",
      "get words #1605\r",
      "get words #1606\r",
      "get words #1607\r",
      "get words #1608\r",
      "get words #1609\r",
      "get words #1610\r",
      "get words #1611\r",
      "get words #1612\r",
      "get words #1613\r",
      "get words #1614\r",
      "get words #1615\r",
      "get words #1616\r",
      "get words #1617\r",
      "get words #1618\r",
      "get words #1619\r",
      "get words #1620\r",
      "get words #1621\r",
      "get words #1622\r",
      "get words #1623\r",
      "get words #1624\r",
      "get words #1625\r",
      "get words #1626\r",
      "get words #1627\r",
      "get words #1628\r",
      "get words #1629\r",
      "get words #1630\r",
      "get words #1631\r",
      "get words #1632\r",
      "get words #1633\r",
      "get words #1634\r",
      "get words #1635\r",
      "get words #1636\r",
      "get words #1637\r",
      "get words #1638\r",
      "get words #1639\r",
      "get words #1640\r",
      "get words #1641\r",
      "get words #1642\r",
      "get words #1643\r",
      "get words #1644\r",
      "get words #1645\r",
      "get words #1646\r",
      "get words #1647\r",
      "get words #1648"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get words #55777\n",
      "total words: 55779\n",
      "sentence count #200000\n",
      "load model ...\n",
      "save csv ...\n",
      "Finish Predicting\n"
     ]
    }
   ],
   "source": [
    "# 開始測試模型並做預測\n",
    "print(\"loading testing data ...\")\n",
    "test_x = load_testing_data(testing_data)\n",
    "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
    "embedding = preprocess.make_embedding(load=True)\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "test_dataset = TwitterDataset(X=test_x, y=None)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            ) #num_workers = 8\n",
    "print('\\nload model ...')\n",
    "model = torch.load(os.path.join('./hw4/logs', 'ckpt.model'))\n",
    "outputs = testing(batch_size, test_loader, model, device)\n",
    "\n",
    "# 寫到csv檔案供上傳kaggle\n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
    "print(\"save csv ...\")\n",
    "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
    "print(\"Finish Predicting\")\n",
    "\n",
    "# 以下是使用command line上傳到kaggle的方式\n",
    "# 需要先pip install kaggle、Create API Token，詳細請看https://github.com/Kaggle/kaggle-api以及https://www.kaggle.com/code1110/how-to-submit-from-google-colab\n",
    "# kaggle competitions submit [competition-name] -f [csv file path]] -m [message]\n",
    "# ex: kaggle competitions submit ml-2020spring-hw4 -f output/predict.csv -m \"......\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSvgTRuGu2Qb"
   },
   "source": [
    "#### Check where the files are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "8SZYJQ62utiK",
    "outputId": "f42a12be-91af-49fa-d4ce-d300d16dc812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "ckpt.model   testing_data.txt\t   w2v_all.model.trainables.syn1neg.npy\n",
      "data.zip     training_label.txt    w2v_all.model.wv.vectors.npy\n",
      "predict.csv  training_nolabel.txt\n",
      "sample_data  w2v_all.model\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iCyLSWFcEmP"
   },
   "source": [
    "### Run 20 epochs on n98\n",
    "real\t3m33.317s\n",
    "\n",
    "user\t3m29.813s\n",
    "\n",
    "sys\t1m9.469s"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "hw4_RNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
